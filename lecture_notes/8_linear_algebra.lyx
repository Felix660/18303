#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Numerical Linear Algebra for PDEs
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Ax=f
\]

\end_inset


\end_layout

\begin_layout Section
Thomas Algorithm
\end_layout

\begin_layout Standard
Let's take a little bit of a dive into how this would be concretely solved.
 In our case, 
\begin_inset Formula $A$
\end_inset

 isn't just any old matrix, it's a tridiagonal matrix (with maybe a few
 points on the corners if the BCs are periodic).
 Wilson's algorithm is an efficient method 
\begin_inset Formula $\left(\mathcal{O}(n)\right)$
\end_inset

 for 
\begin_inset Formula $Au=f$
\end_inset

 when 
\begin_inset Formula $A$
\end_inset

 is tridiagonal.
 Let's see how this works.
 
\end_layout

\begin_layout Standard
Look at the equation as
\begin_inset Formula 
\[
a_{i}x_{i-1}+b_{i}x_{i}+c_{i}x_{i+1}=d_{i}
\]

\end_inset

 Now re-write this in terms of 
\begin_inset Formula $x_{i-1}$
\end_inset

.
 At the start you have
\begin_inset Formula 
\[
b_{1}x_{1}+c_{1}x_{2}=d_{1}
\]

\end_inset

 so
\begin_inset Formula 
\[
x_{1}=\frac{d_{1}-c_{1}x_{2}}{b_{1}}
\]

\end_inset

 Now use this equation to eliminate 
\begin_inset Formula $x_{1}$
\end_inset

 from equation two, and then solve for 
\begin_inset Formula $x_{2}$
\end_inset

.
 You can do this all the way down to the end, and equation 
\begin_inset Formula $n-1$
\end_inset

 gives you an equation for 
\begin_inset Formula $x_{n-1}$
\end_inset

 in terms of 
\begin_inset Formula $x_{n}$
\end_inset

.
 But now the last equation is
\begin_inset Formula 
\[
b_{n}x_{n-1}+c_{n}x_{n}=d_{n}
\]

\end_inset

so you plug in your equation for 
\begin_inset Formula $x_{n-1}$
\end_inset

 to eliminate 
\begin_inset Formula $x_{n-1}$
\end_inset

 from the equation, and solve for 
\begin_inset Formula $x_{n}$
\end_inset

.
 Once you have this value, you recursively work back down the chain to calculate
 each of the 
\begin_inset Formula $x_{i}$
\end_inset

.
\end_layout

\begin_layout Section
Factorization Techniques
\end_layout

\begin_layout Standard
Gaussian elimination can instead be thought of as generating two matrices,
 such that
\begin_inset Formula 
\[
A=LU
\]

\end_inset

 have 
\begin_inset Formula $L$
\end_inset

 as lower triangular and 
\begin_inset Formula $U$
\end_inset

 is upper triangular.
 Then backsubstitution is simple: first equation has one unknown, second
 has two, etc.
 and you iterate downwards to solve a lower or upper triangular system.
 Thus
\begin_inset Formula 
\[
LUx=f
\]

\end_inset

 can be easily solved by 
\begin_inset Quotes eld
\end_inset

inverting
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $L$
\end_inset

 and then 
\begin_inset Quotes eld
\end_inset

inverting
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $U$
\end_inset

.
 This is a way of writing down Gaussian elimination without ever explicitly
 creating the inverse matrix 
\begin_inset Formula $A^{-1}$
\end_inset

.
 There are varients of this (sparse LU) which get the lower and upper triangular
 matrices when 
\begin_inset Formula $A$
\end_inset

 is sparse, but this is a nice formulation since 
\begin_inset Formula $A^{-1}$
\end_inset

 would be dense.
\end_layout

\begin_layout Section
Classical Iterative Methods
\end_layout

\begin_layout Standard
The basic idea is
\end_layout

\begin_layout Enumerate
Form the residual 
\begin_inset Formula $r=f-Au_{k}$
\end_inset


\end_layout

\begin_layout Enumerate
Compute the correction 
\begin_inset Formula $e=Br$
\end_inset

 with 
\begin_inset Formula $B\approx A^{-1}$
\end_inset


\end_layout

\begin_layout Enumerate
Obtain the next approximation 
\begin_inset Formula $u_{k+1}=u_{k}+e$
\end_inset


\end_layout

\begin_layout Standard
The operator 
\begin_inset Formula $B$
\end_inset

 is the iterator and is assumed to be nonsingular.
 If we write this out, this means that
\begin_inset Formula 
\[
u_{k+1}=u_{k}+B\left(f-Au_{k}\right)
\]

\end_inset

 or
\begin_inset Formula 
\[
u_{k+1}=\left(I-BA\right)u_{k}+Bf
\]

\end_inset

 That means that if 
\begin_inset Formula $B=A^{-1}$
\end_inset

,
\begin_inset Formula 
\[
u_{k+1}=Bf=A^{-1}f
\]

\end_inset

 is the true solution! So if we instead check against the true solution
 
\begin_inset Formula $u$
\end_inset

, we can get that
\begin_inset Formula 
\[
u-u_{k+1}=\left(I-BA\right)\left(u-u_{k}\right)
\]

\end_inset

 so 
\begin_inset Formula $\left(I-BA\right)$
\end_inset

 is the amplification matrix for the error.
 If we choose 
\begin_inset Formula $B$
\end_inset

 sufficiently close to 
\begin_inset Formula $A$
\end_inset

, then this scheme will make 
\begin_inset Formula $u_{k+1}\rightarrow u$
\end_inset

.
\end_layout

\begin_layout Standard
How do we choose 
\begin_inset Formula $B$
\end_inset

? There are many different possible choices.
 One way of thinking about it is to split 
\begin_inset Formula 
\[
A=D+L+U
\]

\end_inset

 where 
\begin_inset Formula $D$
\end_inset

 is the diagonal, 
\begin_inset Formula $L$
\end_inset

 is the lower triangular matrix, and 
\begin_inset Formula $U$
\end_inset

 is the upper triangular matrix.
 This is done because each part is trivial to invert.
 Popular methods are:
\end_layout

\begin_layout Enumerate
Richardson: 
\begin_inset Formula $B=\omega I$
\end_inset


\end_layout

\begin_layout Enumerate
Jacobi: 
\begin_inset Formula $B=\omega D^{-1}$
\end_inset


\end_layout

\begin_layout Enumerate
Gauss-Seidel: 
\begin_inset Formula $B=\left(D+L\right)^{-1}$
\end_inset


\end_layout

\begin_layout Enumerate
Successive Over-Relaxation: 
\begin_inset Formula $B=\omega\left(D+\omega L\right)^{-1}$
\end_inset

 (with a specific 
\begin_inset Formula $\omega$
\end_inset

 it's very optimized!)
\end_layout

\begin_layout Standard
Using Julia or MATLAB, writing Gauss-Seidel is simple:
\begin_inset Formula 
\[
u=u+tril(A)\backslash(f-A*u)
\]

\end_inset


\end_layout

\begin_layout Subsection
Properties of the method
\end_layout

\begin_layout Enumerate
Compatible with matrix-free operators
\end_layout

\begin_layout Enumerate
Does not require 
\begin_inset Formula $\mathcal{O}\left(n^{3}\right)$
\end_inset

 inversions! (But cost is somewhere else?)
\end_layout

\begin_layout Subsection
Convergence
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $e_{k}=u-u_{k}$
\end_inset

 be the error in the 
\begin_inset Formula $k$
\end_inset

th step.
 Notice that we can write this as:
\begin_inset Formula 
\[
e_{k+1}=\left(I-BA\right)e_{k}=\left(I-BA\right)^{k+1}e_{0}
\]

\end_inset

 and so this converges iff 
\begin_inset Formula $\left(I-BA\right)$
\end_inset

 causes a shrinkage in norm.
 Whether this occurs can happen by looking at the eigenspectrum.
 Then we require that
\begin_inset Formula 
\[
\left|1-\lambda\right|<1
\]

\end_inset

 for all eigenvalues, which gives that
\begin_inset Formula 
\[
\Vert e_{k+1}\Vert=\Vert I-BA\Vert\Vert e_{k}\Vert<\Vert e_{k}\Vert
\]

\end_inset

 With this idea, you can prove some theorems.
\end_layout

\begin_layout Subsubsection
Theorem 1
\end_layout

\begin_layout Standard
Richardson method with 
\begin_inset Formula $B=\omega I$
\end_inset

 converges iff 
\begin_inset Formula $0<\omega<\frac{2}{\lambda_{max}}$
\end_inset

, and the optimal convergence rate is achieved with
\begin_inset Formula 
\[
\omega^{\ast}=\frac{2}{\lambda_{min}+\lambda_{max}}
\]

\end_inset

 The optimal convergence rate is
\begin_inset Formula 
\[
\rho_{\omega^{\ast}}=\frac{\kappa(A)-1}{\kappa(A)+1}
\]

\end_inset

 where 
\begin_inset Formula $\kappa$
\end_inset

 is the condition number of the matrix 
\begin_inset Formula $A$
\end_inset

:
\begin_inset Formula 
\[
\kappa(A)=\frac{\lambda_{max}}{\lambda_{min}}.
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Theorem 2
\end_layout

\begin_layout Standard
Jacobi method converges iff 
\begin_inset Formula $2D-A=D-L-U$
\end_inset

 is a symmetric positive definite matrix.
\end_layout

\begin_layout Standard
Note that a matrix is diagonally dominated if 
\begin_inset Formula $a_{ii}\geq\sum_{j\neq i}\left|a_{ij}\right|$
\end_inset

 for all 
\begin_inset Formula $i$
\end_inset

 and is strictly if for at least one 
\begin_inset Formula $i$
\end_inset

 it's 
\begin_inset Formula $>$
\end_inset

.
 One can prove that symmetric and strictly diagonally dominant is SPD.
\end_layout

\begin_layout Subsubsection
Corollary
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $A$
\end_inset

 is strictly diagonally dominated, then Jacobi iteration always converges.
 Proof is simple.
 
\begin_inset Formula $A=D+L+U$
\end_inset

, so 
\begin_inset Formula $2D-A=D-L-U$
\end_inset

 is diagonally dominated.
\end_layout

\begin_layout Subsubsection
Theorem 3
\end_layout

\begin_layout Standard
Gauss-Seidel always converges.
\end_layout

\begin_layout Subsubsection
Convergence rate of Poisson
\end_layout

\begin_layout Standard
The convergence rate with 
\begin_inset Formula $\Delta x$
\end_inset

 is
\begin_inset Formula 
\[
\rho\leq1-C\Delta x^{2}
\]

\end_inset

 so convergence slows as 
\begin_inset Formula $\Delta x\rightarrow0$
\end_inset

.
\end_layout

\begin_layout Section
Conjugate Gradient and GMRES
\end_layout

\begin_layout Subsection
Krylov Subspaces
\end_layout

\begin_layout Standard
A Krylov subspace is a sequence of nested subspaces
\begin_inset Formula 
\[
K_{k}=\text{span}\left\{ b,Ab,\ldots,A^{k-1}b\right\} 
\]

\end_inset

 Key property: 
\begin_inset Formula $A^{-1}b\in K_{n}$
\end_inset

 (even when 
\begin_inset Formula $K_{n}\neq\mathbb{R}^{n}$
\end_inset

!).
 Why is this? Take the Cayley-Hamilton Theorem:
\begin_inset Formula 
\[
\rho(A)=A^{n}+a_{1}A^{n-1}+\ldots+a_{n}I=0
\]

\end_inset

 where 
\begin_inset Formula $\rho(\lambda)=\det\left(\lambda I-A\right)=\lambda^{n}+a_{1}\lambda^{n-1}+\ldots+a_{n-1}\lambda+a_{n}$
\end_inset

.
 Therefore you move 
\begin_inset Formula $a_{n}$
\end_inset

 over and multiply by 
\begin_inset Formula $A^{-1}b$
\end_inset

 to get
\begin_inset Formula 
\[
A^{-1}b=\frac{1}{-a_{n}}\left(A^{n-1}b+a_{1}A^{n-2}b+\ldots+a_{n-1}b\right)\in K_{n}
\]

\end_inset


\end_layout

\begin_layout Subsection
Conjugate Gradient (CG)
\end_layout

\begin_layout Standard
Assume 
\begin_inset Formula $A$
\end_inset

 is symmetric positive definite.
 While this property won't be directly used in the justification here, it
 is required for convergence of cg, but the general ideas of how cg works
 can then be extended to not require SPD.
 Conjugate gradient starts from the idea of gradient decent, where we want
 to move in the direction that decreases the error the fastest.
 To do gradient decent, we could walk in the direction defined by the residual
 equation 
\begin_inset Formula $Ae_{k}=r_{k}$
\end_inset

, and then
\begin_inset Formula 
\[
\alpha_{k}=\frac{\left(r_{k},r_{k}\right)}{\left(Ar_{k},r_{k}\right)}=\frac{\left(u-u_{k},r_{k}\right)_{A}}{\left(r_{k},r_{k}\right)_{A}}
\]

\end_inset

 would be the norm of the gradient and so we'd walk
\begin_inset Formula 
\[
u_{k+1}=u_{k}+\alpha_{k}r_{k}.
\]

\end_inset

 Notice this method is nonlinear! But since we want to solve a linear equation,
 we can do something slightly smarter.
 Tracing this method back, we can approximate the gradiant method as
\begin_inset Formula 
\[
u_{k+1}=u_{0}+\alpha_{1}r_{1}+\ldots+\alpha_{k}r_{k}.
\]

\end_inset

 Now let 
\begin_inset Formula $\mathbb{V}_{k}=\text{span}\left\{ r_{0},r_{1},\ldots,r_{k}\right\} $
\end_inset

.
 Instead of just walking downhill, let's choose to walk in the direction
 that is orthogonal to all of the residual vectors we had before.
 Math ensues to find a basis for 
\begin_inset Formula $\mathbb{V}_{k}$
\end_inset

 , orthogonalize via Gram-Schmidt, and then get a recursive process.
 The result is
\begin_inset Formula 
\begin{align*}
u_{k+1} & =u_{k}+\alpha_{k}\rho_{k}\\
r_{k+1} & =r_{k}-\alpha_{k}A\rho_{k}\\
\rho_{k+1} & =r_{k+1}+\beta_{k}\rho_{k}
\end{align*}

\end_inset

 where
\begin_inset Formula 
\[
\alpha_{k}=\frac{\left(u-u_{0},\rho_{k}\right)_{A}}{\left(\rho_{k},\rho_{k}\right)_{A}}
\]

\end_inset

 and
\begin_inset Formula 
\[
\beta_{k}=-\frac{\left(r_{k+1},\rho_{k}\right)_{A}}{\left(\rho_{k},\rho_{k}\right)_{A}}
\]

\end_inset

 This is called the conjugate gradient method.
 You can then get more efficient methods for calculating 
\begin_inset Formula $\alpha_{k}$
\end_inset

 and 
\begin_inset Formula $\beta_{k}$
\end_inset

 as well.
\end_layout

\begin_layout Subsubsection
Theorem
\end_layout

\begin_layout Standard
The convergence rate for a symmetric positive definite matrix is
\begin_inset Formula 
\[
2\left(\frac{\sqrt{\kappa(A)}-1}{\sqrt{\kappa(A)}+1}\right)^{k}
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Theorem
\end_layout

\begin_layout Standard
Conjugate gradient is the optimal Krylov subspace method, that is
\begin_inset Formula 
\[
\left\{ \alpha_{k}\right\} =\arg\min\Vert u-u_{k}\Vert.
\]

\end_inset

 You can't choose any better update mechanism!
\end_layout

\begin_layout Subsubsection
Corollary
\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $A^{-1}b\in K_{n}$
\end_inset

 and 
\begin_inset Formula $\left\{ \alpha_{k}\right\} $
\end_inset

 achieves the minimum distance from the true solution in the 
\begin_inset Formula $K_{k}$
\end_inset

, it follows that CG converges in 
\begin_inset Formula $n$
\end_inset

 iterations.
 In fact, CG converges in the number of steps that matches the dimension
 of the Krylov subspace, assuming no numerical issues.
\end_layout

\begin_layout Subsection
Preconditioners
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $B\approx A^{-1}$
\end_inset

.
 Now use the subspace 
\begin_inset Formula $\mathbb{V}_{k}=\text{span}\left\{ Br_{0},Br_{1},\ldots,Br_{k}\right\} $
\end_inset

.
 Do the same analysis and you get the algorithm with
\begin_inset Formula 
\[
\rho_{k+1}=Br_{k+1}+\beta_{k}\rho_{k}
\]

\end_inset

 and a convergence rate of
\begin_inset Formula 
\[
2\left(\frac{\sqrt{\kappa(BA)}-1}{\sqrt{\kappa(BA)}+1}\right)^{k}
\]

\end_inset

 
\begin_inset Formula $B$
\end_inset

 is called a preconditioner.
 If you have a good inverse approximation that is easy to calculate, you
 can decrease the condition number and speedup the convergence.
 Notice that the classical methods are possible preconditioners!
\end_layout

\begin_layout Subsection
Interesting Result
\end_layout

\begin_layout Standard
Nonlinear CG can be interpreted as L-BFGS with 
\begin_inset Formula $m=1$
\end_inset

 memory.
\end_layout

\begin_layout Subsection
GMRES
\end_layout

\begin_layout Standard
All of this required symmetric positive definite.
 An example of how this can fail is on the following problem:
\begin_inset Formula 
\[
A=\left(\begin{array}{cc}
0 & 1\\
-1 & 0
\end{array}\right),\thinspace f=\left(\begin{array}{c}
1\\
1
\end{array}\right),\thinspace x_{0}=0.
\]

\end_inset

Applying CG here gives 
\begin_inset Formula $\alpha_{0}=0$
\end_inset

, 
\begin_inset Formula $x_{1}=x_{0}$
\end_inset

, 
\begin_inset Formula $r_{1}=r_{0}$
\end_inset

, and then division by zero.
 So GMRES recovers the structure of CG but on more general matrices by directly
 aiming for its 
\begin_inset Quotes eld
\end_inset

best
\begin_inset Quotes erd
\end_inset

 properties.
 The goal is to make your residual vector orthogonal to your previous residuals
 and satisfy 
\begin_inset Formula $\arg\min\Vert f-Au_{k}\Vert$
\end_inset

 (least square problem).
 CG did this before automatically with a few (hidden) tricks.
 To do this now, we first compute an orthonormal basis of our Krylov subspace
 by using Arnoldi iteration.
 Actually computing this orthogonal basis can get costly when the Krylov
 subspace gets large, so in practice you only keep 
\begin_inset Formula $m$
\end_inset

 of the history.
 You then get a method that can be applied to any matrix.
 Preconditioning is still a thing to do which increases convergence rates
 
\begin_inset Quotes eld
\end_inset

similarly
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
Iterative solvers allow for solving 
\begin_inset Formula $Au=f$
\end_inset

 without even requiring a matrix 
\begin_inset Formula $A$
\end_inset

.
 Classical linear methods can be slow to converge based on the condition
 number of the matrix, but preconditioned GMRES is a general method (where
 preconditioners can be the classical iteration pieces) for solving such
 an equation.
 In many cases, it can be much faster than doing direct solving via LU or
 QR factorization if the matrix is large.
 Sparse LU factorizations can be faster, but require you know your sparsity
 pattern! GMRES is an easy solution for the general case, or when you get
 lazy.
\end_layout

\end_body
\end_document
